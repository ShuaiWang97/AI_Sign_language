/home/wangsh/DBALwithImgData-main/main.py:10: DeprecationWarning: Please use `gaussian_filter1d` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.
  from scipy.ndimage.filters import gaussian_filter1d
/opt/conda/conda-bld/pytorch_1634272204863/work/aten/src/ATen/native/cuda/Loss.cu:247: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [14,0,0] Assertion `t >= 0 && t < n_classes` failed.
/opt/conda/conda-bld/pytorch_1634272204863/work/aten/src/ATen/native/cuda/Loss.cu:247: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [15,0,0] Assertion `t >= 0 && t < n_classes` failed.
Using device: cuda:0
state_loop:  [True]

---------- Start max_entropy-MC_dropout=True training! ----------
args.dataset is:  ASL_MNIST_imbal
label_num is:  24
model.conv1 :  Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
model is:  ResNet(
  (conv1): Conv1d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Sequential(
    (0): Linear(in_features=512, out_features=128, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.4, inplace=False)
    (3): Linear(in_features=128, out_features=24, bias=True)
  )
)
********** Experiment Iterations: 1/1 **********
X_init.shape:  (48, 1, 28, 28)
y_init.shape:  (48,)
Traceback (most recent call last):
  File "/home/wangsh/DBALwithImgData-main/main.py", line 338, in <module>
    main()
  File "/home/wangsh/DBALwithImgData-main/main.py", line 333, in main
    results = train_active_learning(args, device, datasets)
  File "/home/wangsh/DBALwithImgData-main/main.py", line 176, in train_active_learning
    training_hist, test_score = active_learning_procedure(dataset=args.dataset,
  File "/home/wangsh/DBALwithImgData-main/active_learning.py", line 39, in active_learning_procedure
    learner = ActiveLearner(
  File "/home/wangsh/.conda/envs/dl2021/lib/python3.9/site-packages/modAL/models/learners.py", line 81, in __init__
    super().__init__(estimator, query_strategy,
  File "/home/wangsh/.conda/envs/dl2021/lib/python3.9/site-packages/modAL/models/base.py", line 71, in __init__
    self._fit_to_known(bootstrap=bootstrap_init, **fit_kwargs)
  File "/home/wangsh/.conda/envs/dl2021/lib/python3.9/site-packages/modAL/models/base.py", line 155, in _fit_to_known
    self.estimator.fit(self.X_training, self.y_training, **fit_kwargs)
  File "/home/wangsh/.conda/envs/dl2021/lib/python3.9/site-packages/skorch/classifier.py", line 141, in fit
    return super(NeuralNetClassifier, self).fit(X, y, **fit_params)
  File "/home/wangsh/.conda/envs/dl2021/lib/python3.9/site-packages/skorch/net.py", line 1215, in fit
    self.partial_fit(X, y, **fit_params)
  File "/home/wangsh/.conda/envs/dl2021/lib/python3.9/site-packages/skorch/net.py", line 1174, in partial_fit
    self.fit_loop(X, y, **fit_params)
  File "/home/wangsh/.conda/envs/dl2021/lib/python3.9/site-packages/skorch/net.py", line 1087, in fit_loop
    self.run_single_epoch(dataset_train, training=True, prefix="train",
  File "/home/wangsh/.conda/envs/dl2021/lib/python3.9/site-packages/skorch/net.py", line 1122, in run_single_epoch
    step = step_fn(batch, **fit_params)
  File "/home/wangsh/.conda/envs/dl2021/lib/python3.9/site-packages/skorch/net.py", line 1007, in train_step
    self._step_optimizer(step_fn)
  File "/home/wangsh/.conda/envs/dl2021/lib/python3.9/site-packages/skorch/net.py", line 963, in _step_optimizer
    optimizer.step(step_fn)
  File "/home/wangsh/.conda/envs/dl2021/lib/python3.9/site-packages/torch/optim/optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/home/wangsh/.conda/envs/dl2021/lib/python3.9/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/home/wangsh/.conda/envs/dl2021/lib/python3.9/site-packages/torch/optim/adam.py", line 92, in step
    loss = closure()
  File "/home/wangsh/.conda/envs/dl2021/lib/python3.9/site-packages/skorch/net.py", line 997, in step_fn
    step = self.train_step_single(batch, **fit_params)
  File "/home/wangsh/.conda/envs/dl2021/lib/python3.9/site-packages/skorch/net.py", line 898, in train_step_single
    loss.backward()
  File "/home/wangsh/.conda/envs/dl2021/lib/python3.9/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/wangsh/.conda/envs/dl2021/lib/python3.9/site-packages/torch/autograd/__init__.py", line 154, in backward
    Variable._execution_engine.run_backward(
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
srun: error: r31n4: task 0: Exited with exit code 1
